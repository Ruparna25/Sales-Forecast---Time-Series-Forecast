{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import lightgbm as lgb\nfrom lightgbm import LGBMRegressor as gbm\nfrom sklearn.ensemble import RandomForestRegressor\nfrom tqdm.auto import tqdm\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.preprocessing import OrdinalEncoder\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"29f3bdf5-67cb-41af-a007-c1a86b859029","_cell_guid":"446ad720-0972-49c4-bda4-b7807be71e7d","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = train_data.loc[train_data['store_nbr'] == 3]\ndata","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cross_validation_method(df,method,test_start_date,train_size=365*2,test_size=15,cv_steps=30,window_shift=1):\n    \n    train_indices_list=[]\n    cv_indices_list=[]\n    \n    if method=='sliding':\n        for step in range(cv_steps):\n            train_start_date=(test_start_date - pd.Timedelta(train_size - step,'days')).date()\n            train_end_date=(test_start_date + pd.Timedelta(step - 1, 'days')).date()\n            train_indices_list.append((train_start_date,train_end_date))\n            \n            cv_start_date=(test_start_date + pd.Timedelta(step,'days')).date()\n            cv_end_date=(test_start_date + pd.Timedelta(step+14, 'days')).date()\n            cv_indices_list.append((cv_start_date,cv_end_date))\n            \n    elif method=='expanding':\n        for step in range(cv_steps):\n            train_start_date=(test_start_date - pd.Timedelta(train_size,'days')).date()\n            train_end_date=(test_start_date + pd.Timedelta(step - 1, 'days')).date()\n            train_indices_list.append((train_start_date,train_end_date))\n            \n            cv_start_date=(test_start_date + pd.Timedelta(step,'days')).date()\n            cv_end_date=(test_start_date + pd.Timedelta(step+14, 'days')).date()\n            cv_indices_list.append((cv_start_date,cv_end_date))\n            \n    return train_indices_list, cv_indices_list\n\ncross_validation_method(data,\n                        method='expanding',\n                        test_start_date=pd.datetime(2017,1,1),\n                        train_size=365*3,\n                        test_size=15,\n                        cv_steps=30)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" Data Processing\n\ndef cartesian(df1,df2):\n    #Determine cartesian product of 2 dataframe\n    key='key'\n    while key in df1.columns or key in df2.columns:\n        print(key)\n        key='_'+key\n    key_d={key:0}\n    return pd.merge(\n        df1.assign(**key_d), df2.assign(**key_d), on=key).drop(key,axis=1)\n\n\ndef data_preprocessing(df,holiday_events_df,oil_df,lag_days=[1,7,30],rolling_days=[7,30,60]):\n    \n    start_date=pd.to_datetime(df['date'].agg(['min','max'])['min'])\n    end_date=pd.to_datetime(df['date'].agg(['min','max'])['max'])\n    date_df = pd.DataFrame()\n    date_df['date']=pd.date_range(start=start_date,end=end_date)\n    unique_store_df=pd.DataFrame({'store_nbr':df['store_nbr'].unique()})\n    \n    date_df = cartesian(date_df,unique_store_df)\n    \n    df=date_df.merge(df,how='left',on=['date','store_nbr'])\n    df=df[df['sales']>0.0]\n    \n    enc=OrdinalEncoder()\n    df['family']=enc.fit_transform(df[['family']])\n  \n    #Create date features\n    df['day_of_month']=df['date'].dt.day\n    df['day_of_week']=df['date'].dt.dayofweek\n    df['day_of_year']=df['date'].dt.dayofyear\n    df['month']=df['date'].dt.month\n    df['year']=df['date'].dt.year\n    df['is_weekend']=(df['day_of_week'] > 5).astype(np.int8)\n    \n    #handling null values\n    df['sales']=df.groupby(['store_nbr','day_of_week'])['sales'].ffill()\n    \n    #creating lag features\n    SHIFT = 15\n    for l in lag_days:\n        df['lag_{}'.format(l)]=df.groupby(['store_nbr','family','day_of_week'])['sales'].transform(lambda x: x.shift(SHIFT+l))\n        \n    #creating rolling features\n    for r in rolling_days:\n        df['rolling_mean_{}'.format(r)]=df.groupby(['store_nbr','family','day_of_week'])['sales'].transform(lambda x:x.shift(SHIFT).rolling(r,min_periods=1).mean())\n        \n    #merging oil data\n    oil_df['date']=pd.to_datetime(oil_df['date'])\n    oil_df = oil_df.rename(columns={\"dcoilwtico\": \"oil_price\"})\n    df=df.merge(oil_df,how='left',on='date')\n    \n    #filling in missing values\n    df['oil_price']=df['oil_price'].fillna(axis=0,method='ffill')\n    #to fill data for the first day we will use the mean price from the 2nd day\n    oil_price=df[df['date']=='2013-01-02']['oil_price']\n    oil_price=round(np.mean(oil_price))\n    df['oil_price']=df['oil_price'].fillna(oil_price)\n    \n    #merging holiday data\n    holiday_events_df['date'] = pd.to_datetime(holiday_events_df['date'])\n    holiday_events_df['type']=holiday_events_df['type'].replace(['Transfer','Additional','Bridge','Event'],'Holiday')\n    holiday_events_df=holiday_events_df.drop(['locale','locale_name','description','transferred'],axis=1)\n    holiday_events_df = holiday_events_df.rename(columns={\"type\": \"day_type\"})\n    df=df.merge(holiday_events_df[['date','day_type']],how='left',on='date')\n    df['day_type'].fillna(False, inplace=True)\n    df['day_type']=df['day_type'].astype(bool).astype(int)\n    \n    \n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_CV(df,cv_method,cv_start_date,lgb_params,train_size=365*2,test_size=15,cv_steps=5,window_shift=1,lag_days=[1,7,30],rolling_days=[7,30,60]):\n    \n    train_indices_list, valid_indices_list = cross_validation_method(df,\n                        cv_method,test_start_date=cv_start_date,\n                        train_size=train_size,\n                        test_size=test_size,\n                        cv_steps=cv_steps,window_shift=window_shift)\n    \n    features = df.columns[~df.columns.isin(['sales','date'])]\n    cat_features = ['family','onpromotion','day_of_month', 'day_of_week', 'month', 'year', 'day_of_month', 'day_of_year', 'is_weekend']\n    \n    feature_importance_df=pd.DataFrame()\n    feature_importance_df['name']=features\n    feature_importance_df['imp']=0\n    fold=0\n    \n    train_date_range=[]\n    avg_mape = 0\n    avg_wmape = 0\n    for train_dates, valid_dates in tqdm(zip(train_indices_list,valid_indices_list)):\n        \n        train_date_range = (df['date'] >= pd.to_datetime(train_dates[0])) & (df['date'] <= pd.to_datetime(train_dates[1]))\n        valid_date_range = (df['date'] >= pd.to_datetime(valid_dates[0])) & (df['date'] <= pd.to_datetime(valid_dates[1]))\n        \n        model = gbm(silent=True, verbose=-1)\n        model.set_params(**lgb_params)\n        model.fit(X=df.loc[train_date_range][features],y=df[train_date_range]['sales'],\n                  categorical_feature = cat_features,\n                  eval_set=[(df[valid_date_range][features],df[valid_date_range]['sales'])],\n                  early_stopping_rounds = 2000,\n                  eval_metric = 'mape',                                  #mape - mean absolute percentage error, its a KPI like RMSE\n                  verbose = False)\n        \n        pred_df=pd.DataFrame({'target':df[valid_date_range]['sales'],'pred':model.predict(df[valid_date_range][features])})\n        pred_df['pred']=round(pred_df['pred'],3)\n        pred_df['weight']=round((pred_df['target']/sum(pred_df['target'])),3)\n        pred_df['mape']=round(abs(pred_df['target']-pred_df['pred'])/pred_df['target'],3)\n        pred_df['wmape']=pred_df['weight']*pred_df['mape']\n        mape=pred_df['mape'].mean()\n        wmape=pred_df['wmape'].sum()\n        \n        avg_mape += mape\n        avg_wmape += wmape\n\n        print(mape,wmape)\n        \n        feature_importance_df['imp']+=model.feature_importances_\n        fold += 1\n        \n    feature_importance_df['imp']=feature_importance_df['imp']/fold\n    \n    print(f'Average MAPE: {avg_mape/cv_steps}, Average WMAPE: {avg_mape/cv_steps}')\n    \n    return pred_df, feature_importance_df, mape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lgb params\nlgb_params = {'boosting_type':'gbdt',\n             'objective':'mape',\n             'n_estimators':1000,\n             'learning_rate':0.1,\n             'num_leaves':127,\n             'max_bin':127,\n             'feature_fraction':0.8,\n             'bagging_fraction':0.8,\n             'verbose': -1\n             }\n\n#Preprocessing data -\ndf=data_preprocessing(data,holidays,oil,lag_days = [1, 7, 14],rolling_days =  [7, 30, 60])\n\npred_df, feat_df, mape=model_CV(df,\n                                cv_method = 'sliding',\n                                cv_start_date = pd.datetime(2017,1,1),\n                                lgb_params = lgb_params,\n                                train_size = 365*3,\n                                test_size =15,\n                                cv_steps = 5,\n                                lag_days = [1,2,3,4,5,6,7,14,30],\n                                rolling_days = [7,30,60]\n                               )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb_params = {'boosting_type':'gbdt',\n             'objective':'mape',\n             'n_estimators':100,\n             'learning_rate':0.1,\n             'num_leaves':127,\n             'max_bin':127,\n             'feature_fraction':0.8,\n             'bagging_fraction':0.8,\n             'verbose': -1\n             }\n\n#Preprocessing data -\ndf=data_preprocessing(data,holidays,oil,lag_days = [1, 7, 14],rolling_days =  [7, 30, 60])\n\npred_df, feat_df, mape=model_CV(df,\n                                cv_method = 'sliding',\n                                cv_start_date = pd.datetime(2017,1,1),\n                                lgb_params = lgb_params,\n                                train_size = 365*3,\n                                test_size =15,\n                                cv_steps = 30,\n                                lag_days = [1,2,3,4,5,6,7,14,30],\n                                rolling_days = [7,30,60]\n                               )\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb_params = {'boosting_type':'gbdt',\n             'objective':'mape',\n             'n_estimators':100,\n             'learning_rate':0.1,\n             'num_leaves':127,\n             'max_bin':127,\n             'feature_fraction':0.8,\n             'bagging_fraction':0.8,\n             'verbose': -1\n             }\n\n#Preprocessing data -\ndf=data_preprocessing(data,holidays,oil,lag_days = [1, 7, 14],rolling_days =  [7, 30, 60])\n\npred_df, feat_df, mape=model_CV(df,\n                                cv_method = 'expanding',\n                                cv_start_date = pd.datetime(2017,1,1),\n                                lgb_params = lgb_params,\n                                train_size = 365*3,\n                                test_size =15,\n                                cv_steps = 15,\n                                lag_days = [1,2,3,4,5,6,7,14,30],\n                                rolling_days = [7,30,60]\n                               )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb_params = {'boosting_type':'gbdt',\n             'objective':'mape',\n             'n_estimators':500,\n             'learning_rate':0.1,\n             'num_leaves':127,\n             'max_bin':127,\n             'feature_fraction':0.8,\n             'bagging_fraction':0.8,\n             'verbose': -1\n             }\n\n#Preprocessing data -\ndf=data_preprocessing(data,holidays,oil,lag_days = [1, 7, 14],rolling_days =  [7, 30, 60])\n\npred_df, feat_df, mape=model_CV(df,\n                                cv_method = 'expanding',\n                                cv_start_date = pd.datetime(2017,1,1),\n                                lgb_params = lgb_params,\n                                train_size = 365*3,\n                                test_size =15,\n                                cv_steps = 15,\n                                lag_days = [1,2,3,4,5,6,7,14,30],\n                                rolling_days = [7,30,60]\n                               )","metadata":{},"execution_count":null,"outputs":[]}]}